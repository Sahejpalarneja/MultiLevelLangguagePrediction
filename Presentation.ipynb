{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bd60fae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topic- Multi Level Predictive Language Model\n",
    "\n",
    "### Supervisor - Prof. Grad-Gyenge László György \n",
    "### Email - grad-gyenge.laszlo@bme.hu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fed2d6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Simply speaking, the goal of grammar learning is to learn the right order of words in a sentence. This assumption is the fundamental statement of modern, predictive language models, e.g., USE, BERT, InferSent, GPT-3. Such models are typically trained by involving transfer learning and also provide high quality language models. For more information, see: https://huggingface.co/\n",
    "\n",
    "However, the mentioned techniques are typically unimodal regarding the atomic units of the language. Most models involve word based representation. However, some approaches are based on characters and hyphens. The goal of this project is to broaden this scope and involve different modalities, e.g., characters, hyphens, words, compound words, sentences, POS tags, etc. The goal is to learn how to combine the mentioned modalities. The student will work on a novel architecture of artificial neural networks and will compare its performance to state of the art techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e689d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a805eb5a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What we wanted to do was to combine various uni modals and make one mega model which would hlp us to get a more accurate prediction of the next word.\n",
    "\n",
    "\n",
    "The plan was to \n",
    "1. Make a Natural Language model that would predict the next character the user will enter\n",
    "2. Make a Natural Language model that would predict the next word the user will enter\n",
    "3. Make a POS(Part-of-Speech) tagger model that would tell us what type of word the next word can be(for eg, a nooun, adverb , conjuction ,adjective etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25aa096",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Each student was assigned a seperate NLP model to make the project\n",
    "I worked the NLP model called LSTMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6342c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A short Description about LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ae8003",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can process not only single data points, but also entire sequences of data . For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition, speech recognition and anomaly detection in network traffic or IDSs (intrusion detection systems). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc3222",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTM\n",
    "\n",
    "- One of the biggest problem of recurrent neural networks is the vanishing gradient problem. \n",
    "- It happens when the gradient shrinks during bakcpropagarion. \n",
    "- If it becomes very small, the network stops learning. This mostly happen when long sentences are present. \n",
    "- LSTM networks address this problem by having an inner memory cell to remember important information or forget others. \n",
    "- LSTM has a similar flow as a RNN, it processes data and passes information as it propagates forward. \n",
    "- The difference is in the operations within the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e301ade",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![lstm](https://miro.medium.com/max/770/1*0f8r3Vd-i4ueYND1CUrhMA.png)\n",
    "\n",
    "Structure of an LSTM cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90239091",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step by Step Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbeb762",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1\n",
    "    Any Natural Language model starts with deciding the framework o be used to analyse the data,\n",
    "    create the model,train the model and to evaluate our findings\n",
    "    \n",
    "    I have gone with Tensorflow and Keras as the frameworks to handle the NLP tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324c3e91",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 2\n",
    "    Next I had to decide on the dataset to use for the training of my model, I found that most of he language prediction model used English Literature books for training the model. In my model I have used 'The Adventures of Sherlock Holmes' and 'Metamorphosis' as the datasets. Although I intend to add more books to increase the vocabulary of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bad0aca",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 3\n",
    "    I needed to clean the dataset with unnecssary inputs like punctuations and blank spaces.I created a vocabulary with all the unique words and assinged them a index. We created a Tensor to get all the data foe the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6417ba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 4\n",
    "     All the above steps were same for the three models but the initialization of the models was different , all the models have different kinds of LSTMs and different number of layers and all of them were trained with different number of epochs"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
